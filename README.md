
# ğŸ“œ VÄá¹‡Ä« â€“ Devotional AI Assistant (RAG over ÅšrÄ«la PrabhupÄdaâ€™s Letters)

VÄá¹‡Ä« is a local AI assistant that answers spiritual questions *only* using the original personal letters of **ÅšrÄ«la A.C. Bhaktivedanta Swami PrabhupÄda** (Founder of ISKCON, 1947â€“1977).

This project uses **Retrieval-Augmented Generation (RAG)** to pull relevant excerpts from 6500+ letters and generate spiritually rooted responses â€” no hallucinations, no opinions, just the mood of the spiritual master.

---

## âœ¨ Features

- ğŸ” **RAG pipeline with FAISS** for fast semantic retrieval
- ğŸ§  **Multi-Context Prompting (MCP)** to merge diverse letter insights
- ğŸ“œ **Modes**: Letters Only | Paraphrased | Hybrid
- ğŸ¦™ Uses **Zephyr or Mistral** via **Ollama** for local inference
- ğŸ§± Built with **LangChain**, **FAISS**, **transformers**, and **Tkinter GUI** or CLI

---

---

## ğŸš€ Getting Started

### 1. Clone the repo

```bash
git clone https://github.com/yourusername/vani.git
cd vani
```

### 2. Create a virtual environment

```bash
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

### 4. Load Model using Ollama (Recommended for Local Inference)

Instead of using Hugging Face Transformers, use **[Ollama](https://ollama.com/)** â€” a simple CLI tool that runs LLMs locally.

#### Step 1: Install Ollama  
https://ollama.com/download

#### Step 2: Pull the model (e.g., Zephyr or Mistral)

```bash
ollama pull mistral
# or
ollama pull zephyr
```

#### Step 3: Run the model server

```bash
ollama run mistral
```

> The model will now listen at `http://localhost:11434`

---

### ğŸ§© 5. Update your Python code to use Ollama

Install the Python client:

```bash
pip install ollama
```

Example usage:

```python
def ask_zephyr(prompt):
    result = subprocess.run(
        [r"C:\Program Files\Ollama\ollama.exe", "run", "zephyr"],
        input=prompt.encode(),
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE
    )
    return result.stdout.decode()

def rag_response(user_question):
    # 1. Retrieve top relevant letters
    relevant_letters = query_vani(user_question, top_k=3)

    # 2. Combine letter bodies
    context = "\n\n---\n\n".join([l["body"] for l in relevant_letters])

    # 3. Bhakti-Style Prompt
    prompt = f"""You are speaking only through the actual letters of His Divine Grace A. C. Bhaktivedanta Swami PrabhupÄda.

Below are excerpts from ÅšrÄ«la PrabhupÄdaâ€™s original letters to his disciples. Your only job is to answer the following question **strictly using these letters alone**, either by quoting directly or paraphrasing in the same tone â€” without adding your own opinions or explanations and bro, i just need answers from the letters only not your hallucinations.

If unsure, humbly say: "Kindly study more of ÅšrÄ«la PrabhupÄdaâ€™s letters for further guidance."

--------------------------
{context}
--------------------------

ğŸ™ Question from a sincere devotee:
"{user_question}"

Now respond with humility and clarity, as ÅšrÄ«la PrabhupÄda would through his letters:
"""

    # 4. Ask Zephyr LLM
    return ask_zephyr(prompt)


response = rag_response("How to increase Book distribution?")
print(response)
```

---

## ğŸ§˜ How It Works

1. User enters a spiritual question
2. The query is embedded using a SentenceTransformer
3. FAISS finds top-k most relevant letters
4. MCP combines distinct letters into a single context block
5. Prompt + context fed to the LLM (via Ollama)
6. Output is returned as:
   - ğŸ“œ Letters Only
   - âœï¸ Paraphrased Response
   - ğŸª· Hybrid Mode

---

## ğŸ’» Running the App

### Option 1: Run via CLI

```bash
python Vani_RAG.py
```

### Option 2: Run GUI (Tkinter)

```bash
python gradio_interface_with_LLM.py
```

---

## ğŸ”§ Customization

You can customize:
- `top_k`: Number of letters retrieved
- Prompt templates in `prompts/prompt_templates.py`
- Model choice (Zephyr, Mistral via Ollama)

---

## ğŸ§ª Planned Improvements

- âœ… LoRA / QLoRA fine-tuning on letter tone
- ğŸ•Šï¸ Web frontend (React/Streamlit)
- ğŸ”– Citation-aware responses
- ğŸŒ Language support for Hindi/Bengali/others

---

## ğŸ™ Credits

- Letters from [Vedabase.io](https://vedabase.io) (scraped with respect)
- LLM: [Zephyr via Ollama](https://ollama.com/library)
- FAISS + LangChain + Transformers stack

---

## ğŸ“¬ Want to Contribute?

If you're a bhakta, engineer, or AI enthusiast and want to help extend VÄá¹‡Ä« â€” you're welcome!  
Feel free to open an issue or PR.

---

## ğŸ“˜ License

Open-source under Apache 2.0 (with attribution to ÅšrÄ«la PrabhupÄdaâ€™s legacy)

---

ğŸª· *"The letters of the spiritual master are non-different from his direct instructions."*  
â€” Inspired by ÅšrÄ«la PrabhupÄdaâ€™s mercy
